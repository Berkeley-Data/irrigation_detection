{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z0HiNpYt6zD"
   },
   "source": [
    "# Big Earth Net Preprocessing\n",
    "## Irrigation Capstone Fall 2020\n",
    "### TP Goter\n",
    "\n",
    "This notebook is used to preprocess the GeoTiff files that contain the Sentinel-2 MSI data comprising the BigEarthNet dataset into TFRecords files. It is based on the preprocessing scripts from the BigEarthNet repo, but has been updated to work in Colaboratory with Python3.7+ and TensorFlow 2.3.\n",
    "\n",
    "This version of the preprocessor is for specifically isolating the irrigated and non-irrigated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4841,
     "status": "ok",
     "timestamp": 1601156548878,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "b0r4VAo9eRWa",
    "outputId": "0d884a87-7e43-470a-98f9-0312431f02ba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "#from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#from google.colab import drive\n",
    "#import seaborn as sns\n",
    "#from matplotlib.cm import get_cmap\n",
    "#import folium\n",
    "#import gdal\n",
    "import rasterio\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4836,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "i8AXp5QRfCMR",
    "outputId": "58c67cd6-4e0a-452b-ba6b-2414f9a44ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.2\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFjjrUFAucPm"
   },
   "source": [
    "## Mount Google Drive and Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4830,
     "status": "ok",
     "timestamp": 1601156548880,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "Q5hp7SNtfCvY",
    "outputId": "3db919a6-5cd3-4949-8f0a-0f0e03e62145"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8nvKWoMlfGZ4"
   },
   "outputs": [],
   "source": [
    "#base_path = '/content/gdrive/My Drive/Capstone Project'\n",
    "big_earth_path ='./BigEarthNet-v1.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUcdgz7bugBq"
   },
   "source": [
    "## Create Symbolic Link(s)\n",
    "Set up a symbolic link to allow for easy Python module imports. Then check to make sure the link works (it is a Unix link so check from shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3169,
     "status": "ok",
     "timestamp": 1601121478276,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "lnsWI7TngGOd",
    "outputId": "53e74cb7-9a99-45d0-fc76-c578d06e763f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: bemodels/bigearthnet-models: File exists\n"
     ]
    }
   ],
   "source": [
    "!ln -s './bigearthnet-models/' bemodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 3163,
     "status": "ok",
     "timestamp": 1601121478277,
     "user": {
      "displayName": "Thomas Goter",
      "photoUrl": "",
      "userId": "00883949598941594885"
     },
     "user_tz": 240
    },
    "id": "GGLMTMT_gyvz",
    "outputId": "9cd87b89-0dfc-4dd8-c201-9a89e6ae583a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md           bigearthnet-models  prep_splits.py      tensorflow_utils.py\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m         label_indices.json  \u001b[34msplits\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls bemodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tIho5IV_giss"
   },
   "outputs": [],
   "source": [
    "from bemodels import tensorflow_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK0uOlk8utUU"
   },
   "source": [
    "## Process All of the BigEarthNet data\n",
    "This simple script will loop over all of the subfolders in the BigEarthNet-v1.0 folder. Currently this folder does not contain the entirety of the BigEarthNet Dataset. Due to this issue, the original scripting was modified to run through the train, test, val sets and only process files if they exist. The previous script simply aborted if a file was listed in the train.csv file and was not in the directory.\n",
    "\n",
    "### Note: This processing takes a really long time. \n",
    "We need to determine if there is a better way to get this data ready for ingestion into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "11N4a4EE-0HGNHlfvZEA3G8x3YzUjEkgn"
    },
    "id": "njMMPbVdfW_f",
    "outputId": "b9d26719-73d9-4a29-b160-76b70fae086e"
   },
   "outputs": [],
   "source": [
    "with open('./bigearthnet-models/label_indices.json', 'rb') as f:\n",
    "    label_indices = json.load(f)\n",
    "\n",
    "root_folder = big_earth_path\n",
    "out_folder = './tfrecords'\n",
    "splits = glob(f'./bigearthnet-models/splits/train.csv')\n",
    "\n",
    "# Checks the existence of patch folders and populate the list of patch folder paths\n",
    "folder_path_list = []\n",
    "if not os.path.exists(root_folder):\n",
    "    print('ERROR: folder', root_folder, 'does not exist')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_names_list = []\n",
    "split_names = []\n",
    "for csv_file in splits:\n",
    "    patch_names_list.append([])\n",
    "    split_names.append(os.path.basename(csv_file).split('.')[0])\n",
    "    with open(csv_file, 'r') as fp:\n",
    "        csv_reader = csv.reader(fp, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            patch_names_list[-1].append(row[0].strip())    \n",
    "\n",
    "# tensorflow_utils.prep_tf_record_files(\n",
    "#     root_folder, out_folder, \n",
    "#     split_names, patch_names_list, \n",
    "#     label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patch_names_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2515/269695 [00:26<45:44, 97.34it/s]  "
     ]
    }
   ],
   "source": [
    "irrigated_examples = []\n",
    "nonirrigated_examples = []\n",
    "missing_count = 0\n",
    "for patch_name in tqdm(patch_names_list[0]):\n",
    "    patch_folder_path = os.path.join(root_folder, patch_name)\n",
    "    patch_json_path = os.path.join(\n",
    "                    patch_folder_path, patch_name + '_labels_metadata.json')\n",
    "    try:\n",
    "        with open(patch_json_path, 'rb') as f:\n",
    "                        patch_json = json.load(f)\n",
    "    except:\n",
    "#         print(f'Missing Labels for {patch_name}')\n",
    "        missing_count += 1\n",
    "        continue\n",
    "\n",
    "    if 'Permanently irrigated land' in patch_json['labels']:\n",
    "        irrigated_examples.append(patch_folder_path)\n",
    "    else:\n",
    "        nonirrigated_examples.append(patch_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2375"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(irrigated_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87739"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonirrigated_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv('./bigearthnet-models/splits/positive_test.csv')\n",
    "neg_df = pd.read_csv('./bigearthnet-models/splits/negative_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(irrigated_examples,columns=['file'])\n",
    "neg_df = pd.DataFrame(nonirrigated_examples,columns=['file'])\n",
    "pos_df.to_csv('./bigearthnet-models/splits/positive_test.csv')\n",
    "neg_df.to_csv('./bigearthnet-models/splits/negative_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data sets for finetuning. Make total dataset size divisible by 32 or 64 for easy batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_1_percent = pos_df.sample(frac=0.0135)\n",
    "#pos_df_10_percent = pos_df.sample(frac=0.1346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_df_1_percent))\n",
    "#print(len(pos_df_10_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac_1p = len(pos_df_1_percent)/len(neg_df)\n",
    "#sample_frac_10p = len(pos_df_10_percent)/len(neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_neg_df_1p = neg_df.sample(frac=sample_frac_1p)\n",
    "#subset_neg_df_10p = neg_df.sample(frac=sample_frac_10p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(subset_neg_df_1p))\n",
    "#print(len(subset_neg_df_10p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*2\n",
    "320*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Index: 0\n",
      "Stop Index: 4971\n",
      "Start Index: 4971\n",
      "Stop Index: 9942\n",
      "Start Index: 9942\n",
      "Stop Index: 14913\n",
      "Start Index: 14913\n",
      "Stop Index: 19884\n",
      "Start Index: 19884\n",
      "Stop Index: 24855\n"
     ]
    }
   ],
   "source": [
    "# start_index = 0\n",
    "# stop_index = 0\n",
    "# # for i in range(5):\n",
    "# #     print(f'Start Index: {start_index}')\n",
    "# #     stop_index = len(subset_neg_df)*(i+1)//5\n",
    "# #     print(f'Stop Index: {stop_index}')\n",
    "# #     balanced_df = pd.concat([pos_df, subset_neg_df[start_index:stop_index]])\n",
    "# #     start_index = stop_index\n",
    "# #     # Shuffle the examples\n",
    "# #     balanced_df = balanced_df.sample(frac=1)\n",
    "# #     balanced_df.to_csv(f'./bigearthnet-models/splits/balanced_val{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.concat([pos_df_1_percent, subset_neg_df_1p])\n",
    "# Shuffle the examples\n",
    "balanced_df = balanced_df.sample(frac=1)\n",
    "balanced_df.to_csv(f'./bigearthnet-models/splits/balanced_train_1percent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: creating the split of balanced_train_1percent is started\n",
      " 0/64 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/64 [..............................] - ETA: 17s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/64 [>.............................] - ETA: 10s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/64 [=>............................] - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/64 [==>...........................] - ETA: 6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/64 [===>..........................] - ETA: 5s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:01,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/64 [====>.........................] - ETA: 5s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/64 [=====>........................] - ETA: 4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/64 [======>.......................] - ETA: 4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/64 [=======>......................] - ETA: 4s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:01, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/64 [========>.....................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:01, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/64 [=========>....................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:01, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/64 [==========>...................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/64 [===========>..................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:02, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/64 [============>.................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:02, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/64 [=============>................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:02, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/64 [==============>...............] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:02, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/64 [==============>...............] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:02, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/64 [===============>..............] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:02, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/64 [================>.............] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:02, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/64 [=================>............] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:03, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/64 [==================>...........] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:03, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/64 [===================>..........] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:03, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/64 [====================>.........] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:03, 16.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/64 [=====================>........] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:03, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/64 [======================>.......] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:03, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/64 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [00:03, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/64 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:03, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/64 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:04, 16.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/64 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:04, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/64 [===========================>..] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:04, 15.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/64 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:04, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/64 [============================>.] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:04, 14.17it/s]\n"
     ]
    }
   ],
   "source": [
    "splits = glob(f'./bigearthnet-models/splits/balanced_train_1percent.*')\n",
    "patch_names_list = []\n",
    "split_names = []\n",
    "for csv_file in splits:\n",
    "    patch_names_list.append([])\n",
    "    split_names.append(os.path.basename(csv_file).split('.')[0])\n",
    "    csv_df = pd.read_csv(csv_file)\n",
    "    patch_names_list[-1] = list(csv_df.file)\n",
    "    patch_names_list[-1] = [name.split('/')[-1] for name in patch_names_list[-1]]\n",
    "    \n",
    "\n",
    "tensorflow_utils.prep_tf_record_files(\n",
    "    root_folder, out_folder, \n",
    "    split_names, patch_names_list, \n",
    "    label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/8/SHpGq20T8v/exkRAUj",
   "collapsed_sections": [],
   "name": "preprocess_tfrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
